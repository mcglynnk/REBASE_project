{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REBASE list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources: \n",
    "\n",
    "BeautifulSoup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#making-the-soup\n",
    "\n",
    "MechanicalSoup Documentation: https://mechanicalsoup.readthedocs.io/en/stable/mechanicalsoup.html\n",
    "\n",
    "https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3\n",
    "https://www.datacamp.com/community/tutorials/web-scraping-using-python\n",
    "https://towardsdatascience.com/how-to-web-scrape-with-python-in-4-minutes-bc49186a8460\n",
    "https://docs.python-guide.org/scenarios/scrape/\n",
    "\n",
    "Python Doc for writerows: https://docs.python.org/2/library/csv.html\n",
    "https://sopython.com/canon/97/writing-csv-adds-blank-lines-between-rows/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import csv\n",
    "import mechanicalsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the link: http://rebase.neb.com/cgi-bin/eyearlist?2\n",
    "The python package MechanicalSoup is a combo of mechanize (web scraping package only available for python 2.7) and BeautifulSoup.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#get url\n",
    "page=mechanicalsoup.StatefulBrowser() #primary tool in MechanicalSoup for interfacing with websites\n",
    "page.open('http://rebase.neb.com/cgi-bin/eyearlist?2')\n",
    "url = 'http://rebase.neb.com/cgi-bin/eyearlist?2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to inspect the HTML page and find the tags containing the data we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enz3_ = page.get_current_page().find_all('tr')[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This opens the mechanicalsoup.StatefulBrowser() page, gets contents, finds all 'tr' tags, makes a list of the contents of all 'tr' tags, and assigns them to enz3_ (variable class is bs4.).  The [4:] skips the first four 'tr' tags, which don't contain any enzyme data. /*When I ran the for loop in the script below without [4:], it stops at 1, since the contents of the first 'tr' don't contain an a tag.*\n",
    "First, find the content I want from just one 'tr' tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tr align=\"left\" bgcolor=\"#FFFFFF\" valign=\"top\">\n",
      "<td><font size=\"2\"><a href=\"/rebase/enz/M.EcoBH212I.html\" target=\"enz\">M.EcoBH212I</a></font></td>\n",
      "<td><font size=\"2\">2019</font></td>\n",
      "<td nowrap=\"\"><font size=\"2\">AACNNNNNNGTGC</font></td>\n",
      "<td><font size=\"2\">-</font></td>\n",
      "<td nowrap=\"\"><font size=\"2\">Type I methyltransferase </font></td>\n",
      "</tr>\n"
     ]
    }
   ],
   "source": [
    "enz_first = enz3_[11]\n",
    "print(enz_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M.EcoBH212I\n"
     ]
    }
   ],
   "source": [
    "name=enz_first.td.a.contents[0]\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 AACNNNNNNGTGC Type I methyltransferase \n"
     ]
    }
   ],
   "source": [
    "year=enz_first.td.find_next_siblings(\"td\")[0].font.contents[0]\n",
    "rec_seq=enz_first.td.find_next_siblings(\"td\")[1].font.contents[0]\n",
    "type=enz_first.td.find_next_siblings(\"td\")[3].font.contents[0]\n",
    "print(year,rec_seq,type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indexing is from BeautifulSoup (https://www.crummy.com/software/BeautifulSoup/bs4/doc/#making-the-soup).  Now a for loop can be used to get these values from each element in enz3_.\n",
    "\n",
    "for i in enz_:\n",
    "    names=i.td.a.contents[0]\n",
    "    year=i.td.find_next_siblings(\"td\")[0].font.contents[0]\n",
    "    rec_seq=i.td.find_next_siblings(\"td\")[1].font.contents[0]\n",
    "    rtype=i.td.find_next_siblings(\"td\")[3].font.contents[0]\n",
    "    print(names, year, rec_seq,rtype)\n",
    "    \n",
    "So far, all of the above could be done with just the BeautifulSoup package.  But it would be useful to have the species name from each restriction enzyme.  This data is inside the enzyme name link for each enzyme. MechanicalSoup has functions to open these links and extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "enz_first = enz3_[11]\n",
    "link_enz_first=page.open_relative(\"http://rebase.neb.com\" + enz_first.td.a['href'])\n",
    "xsoup=BeautifulSoup(link_enz_first.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes the MechanicalSoup object 'page' and opens the link in 'page' at index enz_first.td.a['href'].  The actual link is \"rebase/enz/M.EcoBH212I.html\", which doesn't open without a 'http://', so we have to add the string \"http://rebase.neb.com\" to this (only works because enz_first.td.a['href'] also contains a string; enz_first.td.a didn't work because it contains a 'tag' type variable).\n",
    "The last line makes a new BeautifulSoup object from link_enz_first so that we can index and extract data from it.\n",
    "This indexing was a little harder due to the format of the line containing the organism name, for example:\n",
    "\n",
    "<b>\n",
    "        Organism:\n",
    "       </b>\n",
    "       <a href=\"/cgi-bin/onumget?34289\">\n",
    "        <i>\n",
    "         Escherichia\n",
    "        </i>\n",
    "        <i>\n",
    "         coli\n",
    "        </i>\n",
    "        BH212\n",
    "       </a>\n",
    "\n",
    "Luckily, there aren't too many 'i' tags in the page.  We can extract those and index from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<i>Escherichia</i>, <i>coli</i>, <i>Adeno2: </i>, <i>Lambda: </i>, <i>pBR322: </i>, <i>PhiX174: </i>, <i>SV40: </i>]\n"
     ]
    }
   ],
   "source": [
    "c=xsoup.find_all(\"i\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now index to extract 'Escherichia coli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<i>Escherichia</i> --> ['Escherichia'] --> Escherichia\n"
     ]
    }
   ],
   "source": [
    "print(c[0], '-->',\n",
    "c[0].contents, '-->',\n",
    "c[0].contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<i>coli</i> --> ['coli'] --> coli\n"
     ]
    }
   ],
   "source": [
    "print(c[1], '-->',\n",
    "c[1].contents, '-->',\n",
    "c[1].contents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to join them together so it doesn't get written in the end with \"Escherichia\" and \"coli\" to separate columns. The \" \" separator is needed to make the string two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escherichia coli\n"
     ]
    }
   ],
   "source": [
    "Org=c[0].contents[0]+ \" \" +c[1].contents[0]\n",
    "print(Org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the elements extracting the species name can be put into the for loop from above:\n",
    "\n",
    "for i in enz_first:\n",
    "    names=i.td.a.contents[0]\n",
    "    \n",
    "    link_enz_first=page.open_relative(\"http://rebase.neb.com\" + enz_first.td.a['href'])\n",
    "    xsoup=BeautifulSoup(link_enz_first.text, 'lxml')\n",
    "    c=xsoup.find_all(\"i\")\n",
    "    Org=c[0].contents[0]+ \" \" +c[1].contents[0]\n",
    "    \n",
    "    year=i.td.find_next_siblings(\"td\")[0].font.contents[0]\n",
    "    rec_seq=i.td.find_next_siblings(\"td\")[1].font.contents[0]\n",
    "    rtype=i.td.find_next_siblings(\"td\")[3].font.contents[0]\n",
    "    print(names, Org, year, rec_seq,rtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.open('http://rebase.neb.com/cgi-bin/eyearlist?2')\n",
    "enzx = page.get_current_page().find_all('tr')[5:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M.Nve54I Natrinema versiforme 2019 CATTC Type II methyltransferase \n",
      "M.Nve54III Natrinema versiforme 2019 CTAG Type II methyltransferase \n",
      "YenY4I Yersinia enterocolitica 2019 - Type IV restriction enzyme \n"
     ]
    }
   ],
   "source": [
    "for i in enzx:\n",
    "    names=i.td.a.contents[0]\n",
    "    \n",
    "    link_enzx=page.open_relative(\"http://rebase.neb.com\" + i.td.a['href'])\n",
    "    xsoup=BeautifulSoup(link_enzx.text, 'lxml')\n",
    "    c=xsoup.find_all(\"i\")\n",
    "    Org=c[0].contents[0]+ \" \" +c[1].contents[0]\n",
    "    \n",
    "    year=i.td.find_next_siblings(\"td\")[0].font.contents[0]\n",
    "    rec_seq=i.td.find_next_siblings(\"td\")[1].font.contents[0]\n",
    "    rtype=i.td.find_next_siblings(\"td\")[3].font.contents[0]\n",
    "    print(names, Org, year, rec_seq,rtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!  The last thing is to write the data to a csv file.  For this, a with function gets wrapped around the for loop to write each line into the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'enzx.csv'  #creates a csv file\n",
    "with open(filename, 'w' ,newline='') as f: #the newline argument is necessary in python 3 to prevent empty rows between each row in Windows\n",
    "    fieldnames= ['REnz', 'Org', 'Year','Rec_seq','Rtype'] #set header values\n",
    "    w = csv.DictWriter(f,fieldnames=fieldnames) #fieldnames argument is required when using DictWriter (vs csv.writer)\n",
    "    w.writeheader()\n",
    "\n",
    "    for i in enzx:\n",
    "        Enz=i.td.a.contents[0]\n",
    "\n",
    "        link_enzx=page.open_relative(\"http://rebase.neb.com\" + i.td.a['href'])\n",
    "        xsoup=BeautifulSoup(link_enzx.text, 'lxml')\n",
    "        c=xsoup.find_all(\"i\")\n",
    "        Org=c[0].contents[0]+ \" \" +c[1].contents[0]\n",
    "\n",
    "        Year=i.td.find_next_siblings(\"td\")[0].font.contents[0]\n",
    "        Rec_seq=i.td.find_next_siblings(\"td\")[1].font.contents[0]\n",
    "        Rtype=i.td.find_next_siblings(\"td\")[3].font.contents[0]\n",
    "        print(names, Org, year, rec_seq,rtype)\n",
    "        w.writerow({'REnz':Enz, 'Org':Org, 'Year':Year, 'Rec_seq':Rec_seq, 'Rtype':Rtype})\n",
    "        #when using DictWriter, must specify 'column-name':value to indicate which order to write\n",
    "        #the rows in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 'with' with csv.writer is necessary to close() the file.  Writing to csv without 'with' causes the written file to be shorter than expected.\n",
    "\n",
    "For this site, there were 11,568 rows to write to csv.  This took ~85 minutes and generated an 835 kb csv file. \n",
    "\n",
    "That's it!  Now the data can be processed in R."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
